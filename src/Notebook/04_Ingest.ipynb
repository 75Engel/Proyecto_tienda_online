{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparando el proceso de ingesta de datos.\n",
    "\n",
    "En este notebook vamos a realizar el proceso de ingesta de datos nuevos para la base de datos y poder así poder tener información continua para realizar un modelado de datos.\n",
    "\n",
    "En este punto vamos a proceder a trabajar con código que hemos ido desarrollando en los anteriores notebooks.\n",
    "\n",
    "    - Obtención de datos de la página web, pero aplicado a un determinado tiempo, que será periódico.\n",
    "    - Ingeniería básica para preparar los .csv necesarios  para la carga de datos en la BBDD.\n",
    "    - Carga de datos nuevos en la BBDD.\n",
    "\n",
    "Vamos a intentar reducir el codigo aplicado en relación con los notebooks anteriores. Para ello vamos a recurrir a funciones que generaremos a partir de los codigos anteriores y que estarán en el fichero de [**functions.py**](..\\Utils\\functions.py)\n",
    "\n",
    "Es por ello que aquí se verá solo el código sencillo de llamamiento a funciones donde generará primero unos ficheros .csv para después cargar los mismos en la base de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descargando los datos para su ingesta\n",
    "\n",
    "Aquí vamos a desarrollar las funciones necesarias para obtener la información de la página web. Así como las modificaciones que consideramos oportunas para adecuar la información a las tablas creadas en nuestra BBDD.\n",
    "\n",
    "### Primeramente vamos a convertir en funciones lo que tenemos ahora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(os.path.split(os.getcwd())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "from datetime import datetime\n",
    "import spacy\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from Utils import functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Variables'''\n",
    "\n",
    "url_principal=\"https://www.amantis.net/\"                        \n",
    "url_secundaria=url_principal+'productos-amantis/'\n",
    "\n",
    "\n",
    "date=str(datetime.today().strftime('%y%m%d'))\n",
    "folder_ingest=r'.\\Data'\n",
    "\n",
    "ext=r'.csv'\n",
    "scrape='_scrape'\n",
    "file_product=r'\\productos'\n",
    "file_user=r'\\usuarios'\n",
    "file_comment=r'\\comentarios'\n",
    "file_price=r'\\precios'\n",
    "file_tag=r'\\tags'\n",
    "file_ingest_product=file_product+scrape\n",
    "file_ingest_comment=file_comment+scrape\n",
    "\n",
    "'''Mapeo de meses'''\n",
    "dm_mapping={\n",
    "    'enero':1, \n",
    "    'febrero':2, \n",
    "    'marzo':3, \n",
    "    'abril':4, \n",
    "    'mayo':5,\n",
    "    'junio':6, \n",
    "    'julio':7,\n",
    "    'agosto':8, \n",
    "    'septiembre':9, \n",
    "    'octubre':10, \n",
    "    'noviembre':11, \n",
    "    'diciembre':12,\n",
    "} \n",
    "\n",
    "nombre_listas=['amenities','anal','BDSM','femenino','masculino','juguetes','lenceria','muebles']\n",
    "\n",
    "product_ingest=folder_ingest+file_ingest_product+'_'+date+ext\n",
    "comment_ingest=folder_ingest+file_ingest_comment+'_'+date+ext\n",
    "product_engineer=folder_ingest+file_product+'_'+date+ext\n",
    "tag_engineer=folder_ingest+file_tag+'_'+date+ext\n",
    "price_engineer=folder_ingest+file_price+'_'+date+ext\n",
    "comment_engineer=folder_ingest+file_comment+'_'+date+ext\n",
    "user_engineer=folder_ingest+file_user+'_'+date+ext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spider_amantis(url=url_secundaria,product_ingest=product_ingest,comment_ingest=comment_ingest):\n",
    "    \"\"\"\n",
    "    Obtiene las URLs de los productos para luego extraer la información de cada producto.\n",
    "\n",
    "    Input:\n",
    "    - url (str): URL base para la extracción de productos.\n",
    "\n",
    "    Return:\n",
    "    - noduplicated_product (pd.DataFrame): DataFrame de productos sin duplicados.\n",
    "    - noduplicated_comments (pd.DataFrame): DataFrame de comentarios sin duplicados.\n",
    "    \"\"\"\n",
    "    \n",
    "    pages= np.arange(1,25)  \n",
    "    '''Listas a generar con la información de los productos'''\n",
    "    lista_URLs = []\n",
    "    name=[]\n",
    "    regular_prices=[]\n",
    "    new_price=[]\n",
    "    info=[]\n",
    "    id=[]\n",
    "    comentarios=[]\n",
    "    '''Generamos 2 diccionarios con los datos importantes para ingresar en una BBDD'''\n",
    "\n",
    "    diccionario_datos_productos={\"ID\":id,\"NAME\":name,\"INFO\":info,\"LISTA_URL\":lista_URLs,\"REGULAR_PRICE\":regular_prices,\"DISCOUNT_PRICE\":new_price}\n",
    "\n",
    "    diccionario_comentarios_productos={\"ID\":id,\"COMENTARIOS\":comentarios}\n",
    "\n",
    "    '''Listas para generar la información de los comentarios'''\n",
    "    id_comment=[]\n",
    "    comments=[]\n",
    "    date=[]\n",
    "    ratio=[]\n",
    "    users=[]\n",
    "    comment=[]\n",
    "\n",
    "    print(\"Empezando a recoger datos de las paginas\")\n",
    "    for page in pages:\n",
    "        \n",
    "        if page == 1:\n",
    "            URL = url\n",
    "            response = requests.get(url)\n",
    "            soup = bs(response.text, 'lxml')\n",
    "            productos = soup.find_all(class_='caption')\n",
    "            for producto in productos[9:]:\n",
    "                URL_producto = producto.find('a')['href']\n",
    "                lista_URLs.append(URL_producto)\n",
    "            \n",
    "        else:\n",
    "            URL = url+'page' + str(page)+'/'\n",
    "            response = requests.get(URL)\n",
    "            soup = bs(response.text, 'lxml')\n",
    "            productos = soup.find_all(class_='caption')\n",
    "            for producto in productos[9:]:\n",
    "                URL_producto = producto.find('a')['href']\n",
    "                lista_URLs.append(URL_producto)\n",
    "    print(\"Terminando de recoger los datos de los links de las paginas\\nEmpezando a generar las listas de los productos\")\n",
    "\n",
    "    for i in range(len(lista_URLs)):\n",
    "        id.append(i)\n",
    "\n",
    "        \n",
    "    '''Extraemos la información de cada producto existente'''\n",
    "\n",
    "    for URL in lista_URLs:\n",
    "        url_product=URL\n",
    "        response_product = requests.get(url_product)\n",
    "        soup_product = bs(response_product.text, 'lxml')\n",
    "        user_comments_product=[]\n",
    "        date_comments_product=[]\n",
    "        comments_product=[]\n",
    "        rating=[]\n",
    "\n",
    "        titulos=soup_product.find_all(\"h1\",class_=\"h3\")\n",
    "        for titulo in titulos:\n",
    "            nombre=titulo.get_text(strip=True)\n",
    "            name.append(nombre)\n",
    "\n",
    "        all_price = soup_product.find_all(\"div\", class_=\"productoPrecio pull-right tdd_precio\")                        \n",
    "        for price_container in all_price:                                                                    \n",
    "            try:\n",
    "                special_price = price_container.find(\"span\", class_=\"productSpecialPrice\")\n",
    "                if special_price:\n",
    "                    item_price = float(special_price.get_text(strip=True).replace(\",\", \".\").split('€')[0])\n",
    "                    new_price.append(item_price)\n",
    "                    regular_price = price_container.find(\"del\").get_text(strip=True)\n",
    "                    item_regular_price = float(regular_price.replace(\",\", \".\").split('€')[0])\n",
    "                    regular_prices.append(item_regular_price)\n",
    "                else:\n",
    "                    regular_price = price_container.find(\"span\").get_text(strip=True)\n",
    "                    item_regular_price = float(regular_price.replace(\",\", \".\").split('€')[0])\n",
    "                    new_price.append(item_regular_price)\n",
    "                    regular_prices.append(None)\n",
    "            except:\n",
    "                new_price.append(None)\n",
    "                regular_prices.append(None)\n",
    "\n",
    "        description=soup_product.find(\"div\", class_=\"description\") \n",
    "        information=description.get_text().split('\\n')[1:]\n",
    "        documentation = ''.join(information)\n",
    "        info.append(documentation)\n",
    "\n",
    "\n",
    "        '''Vamos a obtener los datos de los comentarios de los usuarios'''\n",
    "        # print(\"Cargando los datos de los comentarios\")\n",
    "\n",
    "        all_user_comments = soup_product.find_all(\"span\", class_=\"name-user\") \n",
    "        for user_comment in all_user_comments:\n",
    "            user_comments_product.append(user_comment.get_text(strip=True))\n",
    "\n",
    "        \n",
    "        all_dates = soup_product.find_all(\"span\", class_=\"date\")  \n",
    "        for dates in all_dates:\n",
    "            dates_text=dates.get_text(strip=True)\n",
    "            # dates=datetime.strftime(dates, '%dd/%mm/%Y')\n",
    "            date_comments_product.append(dates_text)\n",
    "            # date_object = datetime.strptime(date_comments_product)\n",
    "\n",
    "        all_comments = soup_product.find_all(\"p\")\n",
    "        for formats in all_comments[-len(date_comments_product):]:\n",
    "            comments_product.append(formats.get_text(strip=True))\n",
    "\n",
    "        hearts = soup_product.find_all('div', class_= 'box-description')\n",
    "        for heart in hearts:\n",
    "            heart_rating = heart.find_all('span', class_= 'fas fa-heart')\n",
    "            num_hearts = len(heart_rating)\n",
    "            rating.append(num_hearts)\n",
    "\n",
    "        datos = list(zip(date_comments_product,rating, user_comments_product,comments_product ))\n",
    "        comentarios.append(datos)\n",
    "\n",
    "    for i, regular_price in enumerate(regular_prices):\n",
    "        if regular_price is None:\n",
    "            regular_prices[i] = new_price[i]\n",
    "\n",
    "    print(\"Realizando la ingenieria de los datos\\nEliminando duplicados de productos\")\n",
    "    productos=pd.DataFrame(diccionario_datos_productos)\n",
    "    noduplicated_product = productos.drop_duplicates(subset='NAME', keep='first')\n",
    "    removed_id = productos[productos.duplicated(subset='NAME', keep='first')]['ID']\n",
    "\n",
    "    print(\"Tratando los comentarios\")\n",
    "\n",
    "    comentarios_productos=pd.DataFrame(diccionario_comentarios_productos)\n",
    "    comentarios=pd.DataFrame()\n",
    "    diccionario={\"id\":id_comment,\"comments\":comments}\n",
    "\n",
    "    for id_product,n_comments in enumerate (comentarios_productos['COMENTARIOS']):\n",
    "        for i in n_comments:\n",
    "            id_comment.append(id_product)\n",
    "            comments.append(i)\n",
    "\n",
    "\n",
    "    for j in range(len(diccionario['comments'])):\n",
    "        date.append(diccionario['comments'][j][0])\n",
    "        ratio.append(diccionario['comments'][j][1])\n",
    "        users.append(diccionario['comments'][j][2])\n",
    "        comment.append(diccionario['comments'][j][3])\n",
    "\n",
    "\n",
    "    comentarios['ID']=pd.Series(id_comment)\n",
    "    comentarios['DATE']=pd.Series(date)\n",
    "    comentarios['RATIO']=pd.Series(ratio)\n",
    "    comentarios['USERS']=pd.Series(users)\n",
    "    comentarios['COMMENT']=pd.Series(comment)\n",
    "    \n",
    "    noduplicated_comments = comentarios[~comentarios['ID'].isin(productos[productos['ID'].isin(removed_id)]['ID'])]\n",
    "    \n",
    "    h=input(\"Quieres salvar los datos?\").upper()\n",
    "    if h==\"SI\":\n",
    "        noduplicated_product.to_csv(product_ingest,header=True,index=False)           # Tengo que generar el path correcto\n",
    "        noduplicated_comments.to_csv(comment_ingest,header=True,index=True)           # Tengo que generar el path correcto\n",
    "    \n",
    "    return noduplicated_product,noduplicated_comments\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Generando nuevos dataframes'''\n",
    "def product_engineer(df_product,product_engineer=product_engineer):\n",
    "    \"\"\"\n",
    "    Realiza ingeniería de datos para la creación de la tabla de productos.\n",
    "\n",
    "    Input:\n",
    "    - df_product (pd.DataFrame): DataFrame de productos.\n",
    "\n",
    "    Return:\n",
    "    - df_product (pd.DataFrame): DataFrame de productos con ingeniería aplicada.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Generando el fichero de productos\")\n",
    "    \n",
    "    # df_product=pd.read_csv(folder_ingest+file_ingest_product+'_'+date+ext)\n",
    "    df_product['NAME'] = df_product['NAME'].str.replace(r'-(?=\\w)', '_')\n",
    "    df_product[['PRODUCT', 'SLOGAN']] = df_product['NAME'].str.split('[,-.]', 1, expand=True)\n",
    "    df_product['PRODUCT'] = df_product['PRODUCT'].str.strip()\n",
    "    df_product['SLOGAN'] = df_product['SLOGAN'].str.strip()\n",
    "    df_product['CHARACTERISTICS'] = df_product['INFO'].str.split('Ver características y medidas|Características', 1).str[1]\n",
    "    df_product['DESCRIPTION'] = df_product['INFO'].str.split('Ver características y medidas|Características', 1).str[0].str.strip()\n",
    "    df_product['CHARACTERISTICS'] = df_product['CHARACTERISTICS'].str.replace('\\r', ' ')\n",
    "    df_product['DESCRIPTION'] = df_product['DESCRIPTION'].str.replace('\\r', ' ')\n",
    "\n",
    "    col_1 = df_product.pop('PRODUCT')\n",
    "    col_2=df_product.pop('SLOGAN')\n",
    "    col_3=df_product.pop('DESCRIPTION')\n",
    "    col_4=df_product.pop('CHARACTERISTICS')\n",
    "\n",
    "    df_product.drop(columns=['NAME'],inplace=True)\n",
    "    df_product.drop(columns=['INFO'],inplace=True)\n",
    "\n",
    "    df_product.insert(loc= 1 , column= 'PRODUCT', value= col_1)\n",
    "    df_product.insert(loc= 2 , column= 'SLOGAN', value= col_2)\n",
    "    df_product.insert(loc= 3 , column= 'DESCRIPTION', value= col_3)\n",
    "    df_product.insert(loc= 4 , column= 'CHARACTERISTICS', value= col_4)\n",
    "    df_product=df_product.iloc[:,:6]\n",
    "    # df_product.to_csv(folder_ingest+file_product+'_'+date+ext,header=True,index=False)\n",
    "    h=input(\"Quieres salvar los datos?\").upper()\n",
    "    if h==\"SI\":\n",
    "        df_product.to_csv(product_engineer,header=True,index=False)           # Tengo que generar el path correcto\n",
    "        \n",
    "    return df_product\n",
    "\n",
    "def tag_engineer(df_tag,nombre_listas=nombre_listas,tag_engineer=tag_engineer):\n",
    "    \"\"\"\n",
    "    Realiza ingeniería de datos para la creación de la tabla de tags.\n",
    "\n",
    "    Input:\n",
    "    - df_tag (pd.DataFrame): DataFrame de productos.\n",
    "    - nombre_lista (Lista): Lista de nombres utilizada para cargar listas desde pickles.\n",
    "\n",
    "    Return:\n",
    "    - df_tag (pd.DataFrame): DataFrame de tags de los diversos productos.\n",
    "    \"\"\"\n",
    "    from Utils.functions import eliminacion_acentos,cargar_listas_desde_pickles,aplicar_funcion_a_columna\n",
    "    print(\"Generando el fichero de tags\")\n",
    "    # df_tag=pd.read_csv(folder_ingest+file_product+'_'+date+ext)\n",
    "    df_tag['SLOGAN'] = df_tag['SLOGAN'].str.lower()\n",
    "    df_tag['DESCRIPTION'] = df_tag['DESCRIPTION'].str.lower()\n",
    "    # df_tag['SLOGAN'] = df_tag['SLOGAN'].apply(eliminacion_acentos)              # Esto da error, debe de ser porque hay NaN en el campo\n",
    "    df_tag['DESCRIPTION'] = df_tag['DESCRIPTION'].apply(eliminacion_acentos)\n",
    "    listas = cargar_listas_desde_pickles(nombre_listas)\n",
    "    for nombre_lista in listas:\n",
    "        df_tag = aplicar_funcion_a_columna(df_tag, listas,nombre_lista)\n",
    "    h=input(\"Quieres salvar los datos?\").upper()\n",
    "    if h==\"SI\":\n",
    "        df_tag.to_csv(tag_engineer,header=True,index=False)           # Tengo que generar el path correcto\n",
    "\n",
    "    return df_tag\n",
    "\n",
    "def price_engineer (dataframe,price_engineer=price_engineer):\n",
    "    \"\"\"\n",
    "    Realiza ingeniería de datos para la creación de la tabla de precios.\n",
    "\n",
    "    Input:\n",
    "    - dataframe (pd.DataFrame): DataFrame de productos.\n",
    "\n",
    "    Return:\n",
    "    - df_prices (pd.DataFrame): DataFrame de precios de los productos.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Generando el fichero de precios\")\n",
    "    # dataframe=pd.read_csv(folder_ingest+file_ingest_product+'_'+date+ext)\n",
    "    col_1 = dataframe.pop('REGULAR_PRICE')\n",
    "    col_2=dataframe.pop('DISCOUNT_PRICE')\n",
    "    col_3=dataframe['ID']\n",
    "    dataframe.insert(loc= 1 , column= 'ID_PRODUCT', value= col_3)\n",
    "    dataframe.insert(loc= 2 , column= 'REGULAR_PRICE', value= col_1)\n",
    "    dataframe.insert(loc= 3 , column= 'DISCOUNT_PRICE', value= col_2)\n",
    "    date_price=datetime.today().strftime('%y/%m/%d')\n",
    "    df_prices=dataframe.iloc[:,:4]\n",
    "    df_prices['FECHA']=date_price\n",
    "    h=input(\"Quieres salvar los datos?\").upper()\n",
    "    if h==\"SI\":\n",
    "        df_prices.to_csv(price_engineer,header=True,index=False)           # Tengo que generar el path correcto\n",
    "    return df_prices\n",
    "\n",
    "def comments_engineer (dataframe,dm_mapping=dm_mapping,comment_engineer=comment_engineer,user_engineer=user_engineer):\n",
    "    \"\"\"\n",
    "    Realiza ingeniería de datos para la creación de la tabla de comentarios.\n",
    "\n",
    "    Input:\n",
    "    - dataframe (pd.DataFrame): DataFrame de comentarios.\n",
    "    - dm_mapping (dict): Mapeo de meses para el tratamiento de fechas.\n",
    "\n",
    "    Return:\n",
    "    - df_comments (pd.DataFrame): DataFrame de comentarios con ingeniería aplicada.\n",
    "    - df_users (pd.DataFrame): DataFrame de usuarios que han realizado comentarios.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    '''Tratando a los Usuarios'''\n",
    "    print(\"Generando el fichero de usuarios\")\n",
    "\n",
    "    nombre_count = {}\n",
    "    count = {}\n",
    "\n",
    "    for i, row in dataframe.iterrows():\n",
    "        id = row['ID']\n",
    "        nombre = row['USERS']\n",
    "        \n",
    "        if id not in count:\n",
    "            count[id] = {}\n",
    "            \n",
    "        if nombre in count[id]:\n",
    "            count[id][nombre] += 1\n",
    "            nueva_nombre = f\"{nombre}_{count[id][nombre]}\"\n",
    "            dataframe.loc[i, 'USERS'] = nueva_nombre\n",
    "        else:\n",
    "            count[id][nombre] = 1\n",
    "    lista_users=dataframe['USERS'].unique()\n",
    "    mivalor = [ x for x in range(len(lista_users))]             \n",
    "    lista_users=list(lista_users)                                \n",
    "    lista_users_code = {k: v for k, v in zip(lista_users, mivalor)}   \n",
    "    dataframe['ID_USERS']= dataframe['USERS'].map(lista_users_code)\n",
    "    df_users=pd.DataFrame()\n",
    "    df_users['ID_USERS']=dataframe['ID_USERS']\n",
    "    df_users['USERS']=dataframe['USERS']\n",
    "    df_users.drop_duplicates(subset='ID_USERS', keep='first',inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Generando el fichero de comentarios\")\n",
    "    '''Tratando a las Fechas'''\n",
    "\n",
    "    dataframe['DAY']=dataframe['DATE'].str.split(' ').str.get(1).astype('Int64')\n",
    "    dataframe['MONTH']=dataframe['DATE'].str.split(' ').str.get(2).str.split(',').str.get(0)\n",
    "    dataframe['YEAR']=dataframe['DATE'].str.split(' ').str.get(-1).astype('Int64')\n",
    "    dataframe['MONTH']=dataframe['MONTH'].map(dm_mapping)\n",
    "    dataframe['DATE'] = pd.to_datetime(dataframe.iloc[:,-3:])\n",
    "    df_comments=dataframe.iloc[:,:-3]\n",
    "\n",
    "    col = df_comments.pop('ID_USERS')\n",
    "    df_comments.drop(columns=['USERS'],inplace=True)\n",
    "    df_comments.insert(loc= 4 , column= 'ID_USERS', value= col)\n",
    "    h=input(\"Quieres salvar los datos?\").upper()\n",
    "    if h==\"SI\":\n",
    "        df_users.to_csv(user_engineer,header=True,index=False)           # Tengo que generar el path correcto\n",
    "        df_comments.to_csv(comment_engineer,header=True,index=False)           # Tengo que generar el path correcto\n",
    "\n",
    "    return df_comments,df_users\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probando las funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empezando a recoger datos de las paginas\n",
      "Terminando de recoger los datos de los links de las paginas\n",
      "Empezando a generar las listas de los productos\n"
     ]
    }
   ],
   "source": [
    "noduplicated_product, noduplicated_comments = spider_amantis(url_secundaria)\n",
    "df_product = product_engineer(noduplicated_product)    \n",
    "# df_product.head()\n",
    "# df_tag=tag_engineer(df_product,nombre_listas)\n",
    "# df_tag.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_product.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_engineer(df_tag,nombre_listas=nombre_listas,tag_engineer=tag_engineer):\n",
    "    \"\"\"\n",
    "    Realiza ingeniería de datos para la creación de la tabla de tags.\n",
    "\n",
    "    Input:\n",
    "    - df_tag (pd.DataFrame): DataFrame de productos.\n",
    "    - nombre_lista (Lista): Lista de nombres utilizada para cargar listas desde pickles.\n",
    "\n",
    "    Return:\n",
    "    - df_tag (pd.DataFrame): DataFrame de tags de los diversos productos.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Generando el fichero de tags\")\n",
    "    # df_tag=pd.read_csv(folder_ingest+file_product+'_'+date+ext)\n",
    "    df_tag['SLOGAN'] = df_tag['SLOGAN'].str.lower()\n",
    "    df_tag['DESCRIPTION'] = df_tag['DESCRIPTION'].str.lower()\n",
    "    # df_tag['SLOGAN'] = df_tag['SLOGAN'].apply(f.eliminacion_acentos)              # Esto da error, debe de ser porque hay NaN en el campo\n",
    "    df_tag['DESCRIPTION'] = df_tag['DESCRIPTION'].apply(f.eliminacion_acentos)\n",
    "    listas = f.cargar_listas_desde_pickles(nombre_listas)\n",
    "    # for nombre_lista in listas:\n",
    "    #     df_tag = f.aplicar_funcion_a_columna(df_tag, listas,nombre_lista)\n",
    "    # h=input(\"Quieres salvar los datos?\").upper()\n",
    "    # if h==\"SI\":\n",
    "    #     df_tag.to_csv(tag_engineer,header=True,index=False)           # Tengo que generar el path correcto\n",
    "\n",
    "    return df_tag,listas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tag,listas=tag_engineer(df_product)\n",
    "df_tag.head()\n",
    "# ,nombre_listas=nombre_listas,tag_engineer=tag_engineer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noduplicated_product, noduplicated_comments = spider_amantis(url_secundaria)\n",
    "noduplicated_product.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hay que tener en cuenta que las funciones que dejamos finalmente en el fichero .py sufren modificaciones sobre estas.**\n",
    "\n",
    "### Modificaciones oportunas para adaptar la información a cargar en la BBDDD.\n",
    "\n",
    "En este sentido tenemos que entender que la BBDD ya tiene determinados productos cargados y, por lo tanto, con su ID establecidas.\n",
    "\n",
    "Es por ello que hay que cambiar las ID de la descarga a las ID existentes.\n",
    "\n",
    "¿Cómo lo vamos a realizar? Vamos a extraer la información de la BBDD en un Dataframe y compararlos para cambiar las ID de los campos *[PRODUCT.product]* y *[USERS.users]*.\n",
    "\n",
    "También hay que tener en cuenta que la descarga hemos obtenido toda la información de *COMMENTS*, por lo que hay que ver cual es la fecha más reciente de un comentario en la BBDD y solo dejar en el dataframe de COMMENTS los comentarios no guardados. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Reduciendo los comentarios.\n",
    "\n",
    "¿Qué librerías necesitaremos? \n",
    "\n",
    "    - Las librerías para llamar a la BBDD.\n",
    "    - Las librerías para el manejo de Dataframes.\n",
    "    - Las librerías para el manejo de Fechas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(os.path.split(os.getcwd())[0])\n",
    "folder=os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Data_science\\\\Javier\\\\Repositorios\\\\Proyecto_tienda_online\\\\src'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime,date\n",
    "import sqlite3\n",
    "from Utils import functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ID</th>\n",
       "      <th>DATE</th>\n",
       "      <th>RATIO</th>\n",
       "      <th>USERS</th>\n",
       "      <th>COMMENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>jueves 29 febrero, 2024</td>\n",
       "      <td>5</td>\n",
       "      <td>Ana Vanessa</td>\n",
       "      <td>Es una pasada, es pequeñito pero tiene mucha p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sábado 21 octubre, 2023</td>\n",
       "      <td>5</td>\n",
       "      <td>ana maria</td>\n",
       "      <td>muy práctico por su pequeño tamaño. vibración ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>domingo 01 octubre, 2023</td>\n",
       "      <td>5</td>\n",
       "      <td>Paula</td>\n",
       "      <td>Es pequeño pero útil, da mucho juego y la pila...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>martes 12 septiembre, 2023</td>\n",
       "      <td>4</td>\n",
       "      <td>FCO JAVIER</td>\n",
       "      <td>Magnifica bala, es muy potente y da mucho jueg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>martes 22 agosto, 2023</td>\n",
       "      <td>5</td>\n",
       "      <td>Daria</td>\n",
       "      <td>Pequeño y potente. Se puede combinar con otros...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  ID                        DATE  RATIO        USERS  \\\n",
       "0           0   0     jueves 29 febrero, 2024      5  Ana Vanessa   \n",
       "1           1   0     sábado 21 octubre, 2023      5    ana maria   \n",
       "2           2   0    domingo 01 octubre, 2023      5        Paula   \n",
       "3           3   0  martes 12 septiembre, 2023      4   FCO JAVIER   \n",
       "4           4   0      martes 22 agosto, 2023      5        Daria   \n",
       "\n",
       "                                             COMMENT  \n",
       "0  Es una pasada, es pequeñito pero tiene mucha p...  \n",
       "1  muy práctico por su pequeño tamaño. vibración ...  \n",
       "2  Es pequeño pero útil, da mucho juego y la pila...  \n",
       "3  Magnifica bala, es muy potente y da mucho jueg...  \n",
       "4  Pequeño y potente. Se puede combinar con otros...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_date=pd.read_csv(r'Data\\comentarios_scrape_240301.csv')\n",
    "df_date.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3609 entries, 0 to 3608\n",
      "Data columns (total 6 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Unnamed: 0  3609 non-null   int64 \n",
      " 1   ID          3609 non-null   int64 \n",
      " 2   DATE        3609 non-null   object\n",
      " 3   RATIO       3609 non-null   int64 \n",
      " 4   USERS       3607 non-null   object\n",
      " 5   COMMENT     3609 non-null   object\n",
      "dtypes: int64(3), object(3)\n",
      "memory usage: 169.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df_date.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm_mapping={\n",
    "    'enero':1, \n",
    "    'febrero':2, \n",
    "    'marzo':3, \n",
    "    'abril':4, \n",
    "    'mayo':5,\n",
    "    'junio':6, \n",
    "    'julio':7,\n",
    "    'agosto':8, \n",
    "    'septiembre':9, \n",
    "    'octubre':10, \n",
    "    'noviembre':11, \n",
    "    'diciembre':12,\n",
    "} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a seleccionar el valor máximo de fecha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "383"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def reengineer_comment(BBDD, df_comment):\n",
    "    '''Reducción del número de registros del dataframe commentarios_scrape para su ingesta en la BBDD\n",
    "    Input:\n",
    "    - BBDD (str): Base de datos utilizada.\n",
    "    - df_comment (Dataframe): Dataframe con los comentarios originales, sin filtrar\n",
    "\n",
    "    Return:\n",
    "    - df_comment_filtered (Dataframe): Dataframe con los comentarios filtrados y las fechas convertidas en Datetime.\n",
    "    '''\n",
    "    print(\"Iniciando la conversión de fecha y la reducción de datos de comentarios\")\n",
    "\n",
    "    ''' Conectamos con la base de datos, extraemos la fecha más reciente y la cerramos'''\n",
    "    conn = sqlite3.connect(BBDD)\n",
    "    cursor = conn.cursor()\n",
    "    query='''SELECT MAX(DATE) FROM COMMENT'''\n",
    "    MAX_date=f.sql_query(query,cursor)\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    fecha_maxima = MAX_date.iloc[0, 0]\n",
    "    fecha_maxima = date.fromisoformat(fecha_maxima)\n",
    "    max_date = pd.to_datetime(fecha_maxima, unit='ns')\n",
    "\n",
    "    '''Cargamos el dataframe y lo preparamos para su filtro'''\n",
    "    df_comment['DAY']=df_comment['DATE'].str.split(' ').str.get(1).astype('Int64')\n",
    "    df_comment['MONTH']=df_comment['DATE'].str.split(' ').str.get(2).str.split(',').str.get(0)\n",
    "    df_comment['YEAR']=df_comment['DATE'].str.split(' ').str.get(-1).astype('Int64')\n",
    "\n",
    "    df_comment['MONTH']=df_comment['MONTH'].map(dm_mapping)\n",
    "    df_comment['DATE'] = pd.to_datetime(df_comment.iloc[:,-3:])\n",
    "    df_comment=df_date.iloc[:,:-3]\n",
    "\n",
    "    '''Realizamos el filtro'''\n",
    "    df_comment_filtered=df_comment[df_date['DATE']> max_date]\n",
    "    print(\"El número de registros a ingresar es:\",len(df_comment_filtered))\n",
    "    return df_comment_filtered\n",
    "\n",
    "df_date_filtered=reengineer_comment(\"Resources/online_shop.db\",df_date)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12. Reindexando ***PRODUCTOS***.\n",
    "\n",
    "¿Qué librerías necesitaremos? \n",
    "\n",
    "    - Las librerías para llamar a la BBDD.\n",
    "    - Las librerías para el manejo de Dataframes.\n",
    "    - Las librerías para el manejo de Fechas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
