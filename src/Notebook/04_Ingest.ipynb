{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparando el proceso de ingesta de datos.\n",
    "\n",
    "En este notebook vamos a realizar el proceso de ingesta de datos nuevos para la base de datos y poder así poder tener información continua para realizar un modelado de datos.\n",
    "\n",
    "En este punto vamos a proceder a trabajar con código que hemos ido desarrollando en los anteriores notebooks.\n",
    "\n",
    "    - Obtención de datos de la página web, pero aplicado a un determinado tiempo, que será periódico.\n",
    "    - Ingeniería básica para preparar los .csv necesarios  para la carga de datos en la BBDD.\n",
    "    - Carga de datos nuevos en la BBDD.\n",
    "\n",
    "Vamos a intentar reducir el codigo aplicado en relación con los notebooks anteriores. Para ello vamos a recurrir a funciones que generaremos a partir de los codigos anteriores y que estarán en el fichero de [**functions.py**](..\\Utils\\functions.py)\n",
    "\n",
    "Es por ello que aquí se verá solo el código sencillo de llamamiento a funciones donde generará primero unos ficheros .csv para después cargar los mismos en la base de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "from datetime import datetime\n",
    "import datetime\n",
    "import spacy\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from Utils import functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Variables'''\n",
    "\n",
    "url = \"https://www.amantis.net/productos-amantis/\"              # lista productos\n",
    "url_principal=\"https://www.amantis.net/\"                        # productos\n",
    "pages= np.arange(1,5)\n",
    "\n",
    "date=str(datetime.today().strftime('%y%m%d'))\n",
    "folder_ingest=r'.\\Data'\n",
    "\n",
    "ext=r'.csv'\n",
    "scrape='_scrape'\n",
    "file_product=r'\\productos'\n",
    "file_user=r'\\usuarios'\n",
    "file_comment=r'\\comentarios'\n",
    "file_price=r'\\precios'\n",
    "file_tag=r'\\tags'\n",
    "file_ingest_product=file_product+scrape\n",
    "file_ingest_comment=file_comment+scrape\n",
    "\n",
    "'''Mapeo de meses'''\n",
    "dm_mapping={\n",
    "    'enero':1, \n",
    "    'febrero':2, \n",
    "    'marzo':3, \n",
    "    'abril':4, \n",
    "    'mayo':5,\n",
    "    'junio':6, \n",
    "    'julio':7,\n",
    "    'agosto':8, \n",
    "    'septiembre':9, \n",
    "    'octubre':10, \n",
    "    'noviembre':11, \n",
    "    'diciembre':12,\n",
    "} \n",
    "\n",
    "'''Listas a generar con la información de los productos'''\n",
    "lista_URLs = []\n",
    "name=[]\n",
    "regular_prices=[]\n",
    "new_price=[]\n",
    "info=[]\n",
    "id=[]\n",
    "comentarios=[]\n",
    "\n",
    "\n",
    "'''Generamos 2 diccionarios con los datos importantes para ingresar en una BBDD'''\n",
    "\n",
    "diccionario_datos_productos={\"ID\":id,\"NAME\":name,\"INFO\":info,\"LISTA_URL\":lista_URLs,\"REGULAR_PRICE\":regular_prices,\"DISCOUNT_PRICE\":new_price}\n",
    "\n",
    "diccionario_comentarios_productos={\"ID\":id,\"COMENTARIOS\":comentarios}\n",
    "\n",
    "'''Listas para generar la información de los comentarios'''\n",
    "id_comment=[]\n",
    "comments=[]\n",
    "date=[]\n",
    "ratio=[]\n",
    "users=[]\n",
    "comment=[]\n",
    "\n",
    "nombre_listas=['amenities','anal','BDSM','femenino','masculino','juguetes','lenceria','muebles']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "''' Obtenemos las URLs de los productos para entrar luego en sus URLS y extraer la información'''\n",
    "\n",
    "print(\"Empezando a recoger datos de las paginas\")\n",
    "for page in pages:\n",
    "    \n",
    "    if page == 1:\n",
    "        URL = url\n",
    "        response = requests.get(url)\n",
    "        soup = bs(response.text, 'lxml')\n",
    "        productos = soup.find_all(class_='caption')\n",
    "        for producto in productos[9:]:\n",
    "            URL_producto = producto.find('a')['href']\n",
    "            lista_URLs.append(URL_producto)\n",
    "        \n",
    "    else:\n",
    "        URL = url+'page' + str(page)+'/'\n",
    "        response = requests.get(URL)\n",
    "        soup = bs(response.text, 'lxml')\n",
    "        productos = soup.find_all(class_='caption')\n",
    "        for producto in productos[9:]:\n",
    "            URL_producto = producto.find('a')['href']\n",
    "            lista_URLs.append(URL_producto)\n",
    "print(\"Terminando de recoger los datos de los links de las paginas\\nEmpezando a generar las listas de los productos\")\n",
    "\n",
    "for i in range(len(lista_URLs)):\n",
    "    id.append(i)\n",
    "\n",
    "    \n",
    "'''Extraemos la información de cada producto existente'''\n",
    "\n",
    "for URL in lista_URLs:\n",
    "    url_product=URL\n",
    "    response_product = requests.get(url_product)\n",
    "    soup_product = bs(response_product.text, 'lxml')\n",
    "    user_comments_product=[]\n",
    "    date_comments_product=[]\n",
    "    comments_product=[]\n",
    "    rating=[]\n",
    "\n",
    "    titulos=soup_product.find_all(\"h1\",class_=\"h3\")\n",
    "    for titulo in titulos:\n",
    "        nombre=titulo.get_text(strip=True)\n",
    "        name.append(nombre)\n",
    "\n",
    "    all_price = soup_product.find_all(\"div\", class_=\"productoPrecio pull-right tdd_precio\")                        \n",
    "    for price_container in all_price:                                                                    \n",
    "        try:\n",
    "            special_price = price_container.find(\"span\", class_=\"productSpecialPrice\")\n",
    "            if special_price:\n",
    "                item_price = float(special_price.get_text(strip=True).replace(\",\", \".\").split('€')[0])\n",
    "                new_price.append(item_price)\n",
    "                regular_price = price_container.find(\"del\").get_text(strip=True)\n",
    "                item_regular_price = float(regular_price.replace(\",\", \".\").split('€')[0])\n",
    "                regular_prices.append(item_regular_price)\n",
    "            else:\n",
    "                regular_price = price_container.find(\"span\").get_text(strip=True)\n",
    "                item_regular_price = float(regular_price.replace(\",\", \".\").split('€')[0])\n",
    "                new_price.append(item_regular_price)\n",
    "                regular_prices.append(None)\n",
    "        except:\n",
    "            new_price.append(None)\n",
    "            regular_prices.append(None)\n",
    "\n",
    "    description=soup_product.find(\"div\", class_=\"description\") \n",
    "    information=description.get_text().split('\\n')[1:]\n",
    "    documentation = ''.join(information)\n",
    "    info.append(documentation)\n",
    "\n",
    "\n",
    "    '''Vamos a obtener los datos de los comentarios de los usuarios'''\n",
    "    print(\"Cargando los datos de los comentarios\")\n",
    "\n",
    "    all_user_comments = soup_product.find_all(\"span\", class_=\"name-user\") \n",
    "    for user_comment in all_user_comments:\n",
    "        user_comments_product.append(user_comment.get_text(strip=True))\n",
    "\n",
    "       \n",
    "    all_dates = soup_product.find_all(\"span\", class_=\"date\")  \n",
    "    for dates in all_dates:\n",
    "        dates_text=dates.get_text(strip=True)\n",
    "        # dates=datetime.strftime(dates, '%dd/%mm/%Y')\n",
    "        date_comments_product.append(dates_text)\n",
    "        # date_object = datetime.strptime(date_comments_product)\n",
    "\n",
    "    all_comments = soup_product.find_all(\"p\")\n",
    "    for formats in all_comments[-len(date_comments_product):]:\n",
    "        comments_product.append(formats.get_text(strip=True))\n",
    "\n",
    "    hearts = soup_product.find_all('div', class_= 'box-description')\n",
    "    for heart in hearts:\n",
    "        heart_rating = heart.find_all('span', class_= 'fas fa-heart')\n",
    "        num_hearts = len(heart_rating)\n",
    "        rating.append(num_hearts)\n",
    "\n",
    "    datos = list(zip(date_comments_product,rating, user_comments_product,comments_product ))\n",
    "    comentarios.append(datos)\n",
    "\n",
    "for i, regular_price in enumerate(regular_prices):\n",
    "    if regular_price is None:\n",
    "        regular_prices[i] = new_price[i]\n",
    "\n",
    "print(\"Realizando la ingenieria de los datos\\nEliminando duplicados de productos\")\n",
    "noduplicated_product = productos.drop_duplicates(subset='NAME', keep='first')\n",
    "removed_id = productos[productos.duplicated(subset='NAME', keep='first')]['ID']\n",
    "\n",
    "print(\"Tratando los comentarios\")\n",
    "\n",
    "comentarios_productos=pd.DataFrame(diccionario_comentarios_productos)\n",
    "comentarios=pd.DataFrame()\n",
    "diccionario={\"id\":id_comment,\"comments\":comments}\n",
    "\n",
    "for id_product,n_comments in enumerate (comentarios_productos['COMENTARIOS']):\n",
    "    for i in n_comments:\n",
    "        id_comment.append(id_product)\n",
    "        comments.append(i)\n",
    "\n",
    "\n",
    "for j in range(len(diccionario['comments'])):\n",
    "    date.append(diccionario['comments'][j][0])\n",
    "    ratio.append(diccionario['comments'][j][1])\n",
    "    users.append(diccionario['comments'][j][2])\n",
    "    comment.append(diccionario['comments'][j][3])\n",
    "\n",
    "\n",
    "comentarios['ID']=pd.Series(id_comment)\n",
    "comentarios['DATE']=pd.Series(date)\n",
    "comentarios['RATIO']=pd.Series(ratio)\n",
    "comentarios['USERS']=pd.Series(users)\n",
    "comentarios['COMMENT']=pd.Series(comment)\n",
    "comentarios.info()\n",
    "\n",
    "noduplicated_comments = comentarios[~comentarios['ID'].isin(productos[productos['ID'].isin(removed_id)]['ID'])]\n",
    "noduplicated_comments.info()\n",
    "\n",
    "noduplicated_product.to_csv(folder_ingest+file_ingest_product+'_'+date+ext,header=True,index=False)           # Tengo que generar el path correcto\n",
    "noduplicated_comments.to_csv(folder_ingest+file_ingest_comment+'_'+date+ext,header=True,index=True)           # Tengo que generar el path correcto\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Generando nuevos dataframes'''\n",
    "print(\"Generando el fichero de productos\")\n",
    "dataframe=pd.read_csv(folder_ingest+file_ingest_product+'_'+date+ext)\n",
    "dataframe['NAME'] = dataframe['NAME'].str.replace(r'-(?=\\w)', '_')\n",
    "dataframe[['PRODUCT', 'SLOGAN']] = dataframe['NAME'].str.split('[,-.]', 1, expand=True)\n",
    "dataframe['PRODUCT'] = dataframe['PRODUCT'].str.strip()\n",
    "dataframe['SLOGAN'] = dataframe['SLOGAN'].str.strip()\n",
    "dataframe['CHARACTERISTICS'] = dataframe['INFO'].str.split('Ver características y medidas|Características', 1).str[1]\n",
    "dataframe['DESCRIPTION'] = dataframe['INFO'].str.split('Ver características y medidas|Características', 1).str[0].str.strip()\n",
    "dataframe['CHARACTERISTICS'] = dataframe['CHARACTERISTICS'].str.replace('\\r', ' ')\n",
    "dataframe['DESCRIPTION'] = dataframe['DESCRIPTION'].str.replace('\\r', ' ')\n",
    "\n",
    "col_1 = dataframe.pop('PRODUCT')\n",
    "col_2=dataframe.pop('SLOGAN')\n",
    "col_3=dataframe.pop('DESCRIPTION')\n",
    "col_4=dataframe.pop('CHARACTERISTICS')\n",
    "\n",
    "dataframe.drop(columns=['NAME'],inplace=True)\n",
    "dataframe.drop(columns=['INFO'],inplace=True)\n",
    "\n",
    "dataframe.insert(loc= 1 , column= 'PRODUCT', value= col_1)\n",
    "dataframe.insert(loc= 2 , column= 'SLOGAN', value= col_2)\n",
    "dataframe.insert(loc= 3 , column= 'DESCRIPTION', value= col_3)\n",
    "dataframe.insert(loc= 4 , column= 'CHARACTERISTICS', value= col_4)\n",
    "df_product=dataframe.iloc[:,:6]\n",
    "\n",
    "print(\"Generando el fichero de tags\")\n",
    "df_engineer=pd.read_csv(folder_ingest+file_product+'_'+date+ext)\n",
    "df_engineer['SLOGAN'] = df_engineer['SLOGAN'].str.lower()\n",
    "df_engineer['DESCRIPTION'] = df_engineer['DESCRIPTION'].str.lower()\n",
    "# df_engineer['SLOGAN'] = df_engineer['SLOGAN'].apply(f.eliminacion_acentos)              # Esto da error, debe de ser porque hay NaN en el campo\n",
    "df_engineer['DESCRIPTION'] = df_engineer['DESCRIPTION'].apply(f.eliminacion_acentos)\n",
    "listas = f.cargar_listas_desde_pickles(nombre_listas)\n",
    "for nombre_lista in listas:\n",
    "    df_engineer = f.aplicar_funcion_a_columna(df_engineer, nombre_lista)\n",
    "\n",
    "print(\"Generando el fichero de precios\")\n",
    "df_engineer.to_csv(folder_ingest+file_tag+'_'+date+ext,header=True,index=False)\n",
    "df_product.to_csv(folder_ingest+file_product+'_'+date+ext,header=True,index=False)\n",
    "dataframe=pd.read_csv(folder_ingest+file_ingest_product+'_'+date+ext)\n",
    "col_1 = dataframe.pop('REGULAR_PRICE')\n",
    "col_2=dataframe.pop('DISCOUNT_PRICE')\n",
    "col_3=dataframe['ID']\n",
    "dataframe.insert(loc= 1 , column= 'ID_PRODUCT', value= col_3)\n",
    "dataframe.insert(loc= 2 , column= 'REGULAR_PRICE', value= col_1)\n",
    "dataframe.insert(loc= 3 , column= 'DISCOUNT_PRICE', value= col_2)\n",
    "date_price=datetime.today().strftime('%y/%m/%d')\n",
    "df_prices=dataframe.iloc[:,:4]\n",
    "df_prices['FECHA']=date_price\n",
    "df_prices.to_csv(folder_ingest+file_price+'_'+date+ext,header=True,index=False)\n",
    "\n",
    "print(\"Generando el fichero de comentarios\\ny de usuarios\")\n",
    "df_comments=pd.read_csv(folder_ingest+file_ingest_comment+'_'+date+ext)\n",
    "'''Tratando a las Fechas'''\n",
    "\n",
    "df_comments['DAY']=df_comments['DATE'].str.split(' ').str.get(1).astype('Int64')\n",
    "df_comments['MONTH']=df_comments['DATE'].str.split(' ').str.get(2).str.split(',').str.get(0)\n",
    "df_comments['YEAR']=df_comments['DATE'].str.split(' ').str.get(-1).astype('Int64')\n",
    "df_comments['MONTH']=df_comments['MONTH'].map(dm_mapping)\n",
    "df_comments['DATE'] = pd.to_datetime(df_comments.iloc[:,-3:])\n",
    "df_comments=df_comments.iloc[:,:-3]\n",
    "\n",
    "'''Tratando a los Usuarios'''\n",
    "\n",
    "nombre_count = {}\n",
    "count = {}\n",
    "\n",
    "for i, row in df_comments.iterrows():\n",
    "    id = row['ID']\n",
    "    nombre = row['USERS']\n",
    "    \n",
    "    if id not in count:\n",
    "        count[id] = {}\n",
    "        \n",
    "    if nombre in count[id]:\n",
    "        count[id][nombre] += 1\n",
    "        nueva_nombre = f\"{nombre}_{count[id][nombre]}\"\n",
    "        df_comments.loc[i, 'USERS'] = nueva_nombre\n",
    "    else:\n",
    "        count[id][nombre] = 1\n",
    "lista_users=df_comments['USERS'].unique()\n",
    "mivalor = [ x for x in range(len(lista_users))]             \n",
    "lista_users=list(lista_users)                                \n",
    "lista_users_code = {k: v for k, v in zip(lista_users, mivalor)}   \n",
    "df_comments['ID_USERS']= df_comments['USERS'].map(lista_users_code)\n",
    "df_users=pd.DataFrame()\n",
    "df_users['ID_USERS']=df_comments['ID_USERS']\n",
    "df_users['USERS']=df_comments['USERS']\n",
    "df_users.drop_duplicates(subset='ID_USERS', keep='first',inplace=True)\n",
    "col = df_comments.pop('ID_USERS')\n",
    "df_comments.drop(columns=['USERS'],inplace=True)\n",
    "df_comments.insert(loc= 4 , column= 'ID_USERS', value= col)\n",
    "\n",
    "df_comments.to_csv(folder_ingest+file_comment+'_'+date+ext,header=True,index=False)           # Tengo que generar el path correcto\n",
    "df_users.to_csv(folder_ingest+file_user+'_'+date+ext,header=True,index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
